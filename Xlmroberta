{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Step 1: Preprocess Jigsaw Dataset (Clean, Balance, Split, Tokenize)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer\nfrom datasets import Dataset\n\n# Load dataset from Kaggle input\ndata_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ndf = pd.read_csv(data_path)\n\n# Clean dataset\ndf = df.drop_duplicates(subset=[\"comment_text\"])\ndf = df[[\"comment_text\", \"toxic\"]].dropna()\n\n# Balance classes (optional)\ntoxic_df = df[df[\"toxic\"] == 1]\nnon_toxic_df = df[df[\"toxic\"] == 0].sample(n=len(toxic_df), random_state=42)\nbalanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1, random_state=42)\n\n# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Rename 'toxic' to 'labels' for Trainer compatibility\ntrain_df = train_df.rename(columns={\"toxic\": \"labels\"})\nval_df = val_df.rename(columns={\"toxic\": \"labels\"})\ntest_df = test_df.rename(columns={\"toxic\": \"labels\"})\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Initialize tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenize for each model\ntrain_bert = tokenize_dataset(train_dataset, bert_tokenizer)\nval_bert = tokenize_dataset(val_dataset, bert_tokenizer)\ntest_bert = tokenize_dataset(test_dataset, bert_tokenizer)\n\ntrain_roberta = tokenize_dataset(train_dataset, roberta_tokenizer)\nval_roberta = tokenize_dataset(val_dataset, roberta_tokenizer)\ntest_roberta = tokenize_dataset(test_dataset, roberta_tokenizer)\n\ntrain_distilbert = tokenize_dataset(train_dataset, distilbert_tokenizer)\nval_distilbert = tokenize_dataset(val_dataset, distilbert_tokenizer)\ntest_distilbert = tokenize_dataset(test_dataset, distilbert_tokenizer)\n\n# Save to Kaggle working directory\ntrain_bert.save_to_disk(\"/kaggle/working/preprocessed/train_bert\")\nval_bert.save_to_disk(\"/kaggle/working/preprocessed/val_bert\")\ntest_bert.save_to_disk(\"/kaggle/working/preprocessed/test_bert\")\ntrain_roberta.save_to_disk(\"/kaggle/working/preprocessed/train_roberta\")\nval_roberta.save_to_disk(\"/kaggle/working/preprocessed/val_roberta\")\ntest_roberta.save_to_disk(\"/kaggle/working/preprocessed/test_roberta\")\ntrain_distilbert.save_to_disk(\"/kaggle/working/preprocessed/train_distilbert\")\nval_distilbert.save_to_disk(\"/kaggle/working/preprocessed/val_distilbert\")\ntest_distilbert.save_to_disk(\"/kaggle/working/preprocessed/test_distilbert\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T03:52:40.537529Z","iopub.execute_input":"2025-03-16T03:52:40.537800Z","iopub.status.idle":"2025-03-16T03:54:53.893368Z","shell.execute_reply.started":"2025-03-16T03:52:40.537778Z","shell.execute_reply":"2025-03-16T03:54:53.892441Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f23b71741a4a0a8eca54b496e237b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5492b753454340859e6f0ae68589e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa159b8865684bfba3471e6aa41ae1d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4fdef6cd33481baeaf2f64a3a38c91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbc31a48b594fc9adae968bed99c732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa6adbcffbf42549f17729487d287a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d5fa709b3a1439faa8c9ce5289f0c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7461e41b96c40b9928520549c1760ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12463e9fe9ec4589a0678c79cc267a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935e28c723cd44beab860d04dc731d1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d75a4b19314f6ebece3cd1fe0bfb08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75dabb0433e34360b1a50f01ecf91eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59191167afc94d1d91ba8e0a7ac08480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8c4075aefd4692957ca24939e8da8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40f2214d657471a96f5c7a32bc6000f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89dc20de56a74ec299995335ff61cdc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a164a4126545ecbdc689eb1f486263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1af34dbdc07549df94860f5d7c250207"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b20698e6d640daaa15f4d390e09e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4f1e92f9f64c61b2a6000930a685de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6f3e6b81d0454492a1a5a03eafa03b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b04c5e882c46189f8b3c8ae3445b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407e4567e28e400090cc4e4a71210d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbf7f8abe324122bb5b834274f73604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c14629f71444c0899b6fa69230eeeec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76965e9924af47f78f611477408e278c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2ab3d4cc1b4ca3a219dac112ac56e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4baf33ddc7b42fda13e4ef2602ad5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"defc577d377649979d1043a4671c7fc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d45cae33414f4bbc0b9b9ae79cb7a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa5ee6dd27746ca9afe7a723b53390e"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"Step 2: Fine-Tune BERT, DistilBERT, RoBERTa with Validation Set","metadata":{}},{"cell_type":"code","source":"from transformers import (BertForSequenceClassification, RobertaForSequenceClassification, \n                          DistilBertForSequenceClassification, Trainer, TrainingArguments)\nimport torch\n\n# Load preprocessed datasets\ntrain_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_bert\")\nval_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_bert\")\ntrain_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_roberta\")\nval_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_roberta\")\ntrain_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_distilbert\")\nval_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_distilbert\")\n\n# Load models\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)\n\n# Fine-tune function\ndef fine_tune_model(model, train_dataset, val_dataset, model_name):\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n    trainer.train()\n    trainer.save_model(f\"/kaggle/working/models/{model_name}\")\n    return trainer\n\n# Fine-tune each model\nbert_trainer = fine_tune_model(bert_model, train_bert, val_bert, \"bert_finetuned\")\nroberta_trainer = fine_tune_model(roberta_model, train_roberta, val_roberta, \"roberta_finetuned\")\ndistilbert_trainer = fine_tune_model(distilbert_model, train_distilbert, val_distilbert, \"distilbert_finetuned\")\n\n# Save tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nbert_tokenizer.save_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_tokenizer.save_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_tokenizer.save_pretrained(\"/kaggle/working/models/distilbert_finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T03:57:54.924420Z","iopub.execute_input":"2025-03-16T03:57:54.924726Z","iopub.status.idle":"2025-03-16T04:25:47.473130Z","shell.execute_reply.started":"2025-03-16T03:57:54.924705Z","shell.execute_reply":"2025-03-16T04:25:47.472240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afec2975e274ae690be4994c259d862"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1654d66d2c44186b6547a0bb7cc9da0"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a53fbdfe6d4cc0932a3950baaee96b"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 10:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.239900</td>\n      <td>0.301029</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.151100</td>\n      <td>0.323235</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.074700</td>\n      <td>0.377796</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 11:02, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.261800</td>\n      <td>0.265773</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.176100</td>\n      <td>0.335729</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.132300</td>\n      <td>0.337517</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 05:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.238300</td>\n      <td>0.253399</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.151800</td>\n      <td>0.310935</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.095900</td>\n      <td>0.345024</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/models/distilbert_finetuned/tokenizer_config.json',\n '/kaggle/working/models/distilbert_finetuned/special_tokens_map.json',\n '/kaggle/working/models/distilbert_finetuned/vocab.txt',\n '/kaggle/working/models/distilbert_finetuned/added_tokens.json')"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"Step 3: Evaluate on Test Set and Document Results","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport pandas as pd\nfrom transformers import BertForSequenceClassification, RobertaForSequenceClassification, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load test datasets\ntest_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_bert\")\ntest_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_roberta\")\ntest_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_distilbert\")\n\n# Load fine-tuned models\nbert_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Define training args for evaluation (no training, just eval)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",\n    eval_strategy=\"epoch\",  # Still works since we provide eval_dataset\n    per_device_eval_batch_size=8,\n    report_to=\"none\"\n)\n\n# Compute metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n# Evaluate each model with its test dataset\nbert_trainer = Trainer(\n    model=bert_model,\n    args=training_args,\n    eval_dataset=test_bert,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\nroberta_trainer = Trainer(\n    model=roberta_model,\n    args=training_args,\n    eval_dataset=test_roberta,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\ndistilbert_trainer = Trainer(\n    model=distilbert_model,\n    args=training_args,\n    eval_dataset=test_distilbert,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\n\n# Run evaluation\nbert_eval = bert_trainer.evaluate()\nroberta_eval = roberta_trainer.evaluate()\ndistilbert_eval = distilbert_trainer.evaluate()\n\n# Document results\nresults = {\n    \"Model\": [\"BERT\", \"RoBERTa\", \"DistilBERT\"],\n    \"Accuracy\": [bert_eval[\"eval_accuracy\"], roberta_eval[\"eval_accuracy\"], distilbert_eval[\"eval_accuracy\"]],\n    \"F1\": [bert_eval[\"eval_f1\"], roberta_eval[\"eval_f1\"], distilbert_eval[\"eval_f1\"]],\n    \"Precision\": [bert_eval[\"eval_precision\"], roberta_eval[\"eval_precision\"], distilbert_eval[\"eval_precision\"]],\n    \"Recall\": [bert_eval[\"eval_recall\"], roberta_eval[\"eval_recall\"], distilbert_eval[\"eval_recall\"]]\n}\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/kaggle/working/model_performance.csv\", index=False)\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:08:07.055951Z","iopub.execute_input":"2025-03-05T04:08:07.056248Z","iopub.status.idle":"2025-03-05T04:08:33.993810Z","shell.execute_reply.started":"2025-03-05T04:08:07.056225Z","shell.execute_reply":"2025-03-05T04:08:33.993170Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 4: Test on Curated Toxic/Non-Toxic Examples","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer\n\n# Load tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Load models (already defined in Step 3, just ensuring they're available)\n# If not, reload them here:\nbert_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Move models to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model.to(device)\nroberta_model.to(device)\ndistilbert_model.to(device)\n\n# Curated examples\nexamples = [\n    {\"text\": \"You‚Äôre a complete idiot!\", \"label\": 1},\n    {\"text\": \"Nice work, well done!\", \"label\": 0},\n    {\"text\": \"Die, you worthless scum.\", \"label\": 1},\n    {\"text\": \"This is a great discussion.\", \"label\": 0},\n]\n\n# Prediction function (ensure inputs are on the same device as model)\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n    with torch.no_grad():  # Disable gradient computation for inference\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    return pred, probs[0][1].item()\n\n# Test each model\nfor example in examples:\n    text, true_label = example[\"text\"], example[\"label\"]\n    print(f\"\\nText: {text}\")\n    for model_name, model, tokenizer in [\n        (\"BERT\", bert_model, bert_tokenizer),\n        (\"RoBERTa\", roberta_model, roberta_tokenizer),\n        (\"DistilBERT\", distilbert_model, distilbert_tokenizer),\n    ]:\n        pred, prob = predict_toxicity(model, tokenizer, text, device)\n        print(f\"{model_name}: Predicted={pred}, Toxic Prob={prob:.4f}, True={true_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:13:01.603463Z","iopub.execute_input":"2025-03-05T04:13:01.603786Z","iopub.status.idle":"2025-03-05T04:13:02.508801Z","shell.execute_reply.started":"2025-03-05T04:13:01.603762Z","shell.execute_reply":"2025-03-05T04:13:02.508094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 5: Prepare Models for Web App (Test Inference, Optimize)","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport time\n\n# Use DistilBERT for inference\nclassifier = pipeline(\"text-classification\", model=\"/kaggle/working/models/distilbert_finetuned\", \n                      tokenizer=distilbert_tokenizer)\n\n# Test inference\nstart_time = time.time()\nresult = classifier(\"You‚Äôre a disgusting human being!\")\nend_time = time.time()\nprint(f\"Prediction: {result}, Latency: {end_time - start_time:.4f}s\")\n\n# Additional test for confirmation\nstart_time = time.time()\nresult = classifier(\"Great job, keep it up!\")\nend_time = time.time()\nprint(f\"Prediction: {result}, Latency: {end_time - start_time:.4f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:17:18.311532Z","iopub.execute_input":"2025-03-05T04:17:18.311881Z","iopub.status.idle":"2025-03-05T04:17:18.446165Z","shell.execute_reply.started":"2025-03-05T04:17:18.311812Z","shell.execute_reply":"2025-03-05T04:17:18.445415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip each model folder individually\nshutil.make_archive(\"/kaggle/working/bert_archive\", \"zip\", \"/kaggle/working/models/bert_finetuned\")\nshutil.make_archive(\"/kaggle/working/roberta_archive\", \"zip\", \"/kaggle/working/models/roberta_finetuned\")\nshutil.make_archive(\"/kaggle/working/distilbert_archive\", \"zip\", \"/kaggle/working/models/distilbert_finetuned\")\n\nprint(\"Created bert_archive.zip, roberta_archive.zip, and distilbert_archive.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T04:31:37.599641Z","iopub.execute_input":"2025-03-16T04:31:37.599953Z","iopub.status.idle":"2025-03-16T04:32:40.880744Z","shell.execute_reply.started":"2025-03-16T04:31:37.599932Z","shell.execute_reply":"2025-03-16T04:32:40.879868Z"}},"outputs":[{"name":"stdout","text":"Created bert_archive.zip, roberta_archive.zip, and distilbert_archive.zip\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/distilbert_archive\", \"zip\", \"/kaggle/working/models/distilbert_finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T03:40:33.065636Z","iopub.execute_input":"2025-03-16T03:40:33.065941Z","iopub.status.idle":"2025-03-16T03:40:46.408175Z","shell.execute_reply.started":"2025-03-16T03:40:33.065916Z","shell.execute_reply":"2025-03-16T03:40:46.407471Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/distilbert_archive.zip'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nfor file in [\"bert_archive.zip\", \"roberta_archive.zip\", \"distilbert_archive.zip\"]:\n    file_path = f\"/kaggle/working/{file}\"\n    size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n    print(f\"{file}: {size:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T05:19:49.727369Z","iopub.execute_input":"2025-03-17T05:19:49.727720Z","iopub.status.idle":"2025-03-17T05:19:49.753670Z","shell.execute_reply.started":"2025-03-17T05:19:49.727698Z","shell.execute_reply":"2025-03-17T05:19:49.752479Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-8128fccf8b2d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bert_archive.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"roberta_archive.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"distilbert_archive.zip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/kaggle/working/{file}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Size in MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file}: {size:.2f} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/bert_archive.zip'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/bert_archive.zip'","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"## Multilingual Toxic Comment Classification with XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# Step 6: Fine-Tune XLM-RoBERTa and Test All Models on YouTube Comments\n\nimport torch\nimport pandas as pd\nfrom transformers import (XLMRobertaTokenizer, XLMRobertaForSequenceClassification, \n                          BertForSequenceClassification, BertTokenizer,\n                          RobertaForSequenceClassification, RobertaTokenizer,\n                          DistilBertForSequenceClassification, DistilBertTokenizer,\n                          Trainer, TrainingArguments)\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# --- Part 1: Fine-Tune XLM-RoBERTa ---\n\n# Load Jigsaw dataset\ndata_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ndf = pd.read_csv(data_path)\ndf = df.drop_duplicates(subset=[\"comment_text\"])\ndf = df[[\"comment_text\", \"toxic\"]].dropna()\n\n# Balance classes\ntoxic_df = df[df[\"toxic\"] == 1]\nnon_toxic_df = df[df[\"toxic\"] == 0].sample(n=len(toxic_df), random_state=42)\nbalanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1, random_state=42)\n\n# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Rename 'toxic' to 'labels' for Trainer compatibility\ntrain_df = train_df.rename(columns={\"toxic\": \"labels\"})\nval_df = val_df.rename(columns={\"toxic\": \"labels\"})\ntest_df = test_df.rename(columns={\"toxic\": \"labels\"})\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize with XLM-RoBERTa\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\ntrain_xlm = tokenize_dataset(train_dataset, xlm_tokenizer)\nval_xlm = tokenize_dataset(val_dataset, xlm_tokenizer)\ntest_xlm = tokenize_dataset(test_dataset, xlm_tokenizer)\n\n# Save tokenized datasets\ntrain_xlm.save_to_disk(\"/kaggle/working/preprocessed/train_xlm\")\nval_xlm.save_to_disk(\"/kaggle/working/preprocessed/val_xlm\")\ntest_xlm.save_to_disk(\"/kaggle/working/preprocessed/test_xlm\")\n\n# Fine-tune XLM-RoBERTa\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models/xlm_finetuned\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=xlm_model,\n    args=training_args,\n    train_dataset=train_xlm,\n    eval_dataset=val_xlm,\n)\ntrainer.train()\ntrainer.save_model(\"/kaggle/working/models/xlm_finetuned\")\nxlm_tokenizer.save_pretrained(\"/kaggle/working/models/xlm_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T04:54:50.341145Z","iopub.execute_input":"2025-03-17T04:54:50.341512Z","iopub.status.idle":"2025-03-17T05:10:43.513005Z","shell.execute_reply.started":"2025-03-17T04:54:50.341465Z","shell.execute_reply":"2025-03-17T05:10:43.512249Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"174dabdeae2f44f0a0d88afdbcde4b1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae1af80b4224cb7af0c3d65434ea74d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f011602ec5b54807a0bf4f35061358ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3201503fde410297252a17460590bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc02c5d28c0445fb99761b2f72b74aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dda066ac271424cbfc19fd28c18de67"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 15:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.288000</td>\n      <td>0.263545</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.196400</td>\n      <td>0.337691</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.144200</td>\n      <td>0.355859</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/models/xlm_finetuned/tokenizer_config.json',\n '/kaggle/working/models/xlm_finetuned/special_tokens_map.json',\n '/kaggle/working/models/xlm_finetuned/sentencepiece.bpe.model',\n '/kaggle/working/models/xlm_finetuned/added_tokens.json')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"\n# Test inference on multilingual examples\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\").to(device)\n\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    return pred, probs[0][1].item()\n\n# Multilingual and emoji examples\nexamples = [\n    {\"text\": \"¬°Eres un idiota! üòÇ\", \"label\": 1},  # Spanish\n    {\"text\": \"Tu es g√©nial! üëç\", \"label\": 0},     # French\n    {\"text\": \"‡§Ü‡§™ ‡§Æ‡•Ç‡§∞‡•ç‡§ñ ‡§π‡•à‡§Ç! üò°\", \"label\": 1},     # Hindi\n    {\"text\": \"Great work üòä\", \"label\": 0},        # English with emoji\n]\n\nfor example in examples:\n    text, true_label = example[\"text\"], example[\"label\"]\n    pred, prob = predict_toxicity(xlm_model, xlm_tokenizer, text, device)\n    print(f\"\\nText: {text}\")\n    print(f\"XLM-RoBERTa: Predicted={pred}, Toxic Prob={prob:.4f}, True={true_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T05:30:59.677935Z","iopub.execute_input":"2025-03-17T05:30:59.678300Z","iopub.status.idle":"2025-03-17T05:31:00.138526Z","shell.execute_reply.started":"2025-03-17T05:30:59.678272Z","shell.execute_reply":"2025-03-17T05:31:00.137663Z"}},"outputs":[{"name":"stdout","text":"\nText: ¬°Eres un idiota! üòÇ\nXLM-RoBERTa: Predicted=1, Toxic Prob=0.9855, True=1\n\nText: Tu es g√©nial! üëç\nXLM-RoBERTa: Predicted=0, Toxic Prob=0.0102, True=0\n\nText: ‡§Ü‡§™ ‡§Æ‡•Ç‡§∞‡•ç‡§ñ ‡§π‡•à‡§Ç! üò°\nXLM-RoBERTa: Predicted=1, Toxic Prob=0.9773, True=1\n\nText: Great work üòä\nXLM-RoBERTa: Predicted=0, Toxic Prob=0.0077, True=0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Step 6: Test XLM-RoBERTa on Pre-Collected YouTube Comments\n\nimport torch\nimport pandas as pd\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\n# Load fine-tuned XLM-RoBERTa model and tokenizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\").to(device)\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\n\n# Prediction function\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    toxic_prob = probs[0][1].item()\n    return pred, toxic_prob\n\n# Pre-collected YouTube comments (curated to reflect diversity)\nyoutube_comments = [\n    # English without emojis/symbols\n    \"This video is terrible\",\n    \"Really well made content\",\n    \"You should quit making videos\",\n    \"Thanks for the great upload\",\n    # English with emojis/symbols\n    \"Worst thing ever üòÇüëé\",\n    \"Love this so much! üòç‚ú®\",\n    \"Go jump off a cliff ‚ò†Ô∏è\",\n    \"Awesome work! üòäüëç\",\n    # Multilingual without emojis/symbols\n    \"¬°Qu√© video tan malo!\",\n    \"Bon contenu, merci!\",\n    \"‡§Ü‡§™‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§π‡•à\",\n    \"„Åì„ÅÆ„Éì„Éá„Ç™„ÅØ„Å≤„Å©„ÅÑ\",\n    # Multilingual with emojis/symbols\n    \"¬°Eres un genio! ‚≠ê\",\n    \"Tu es nul üò°üëä\",\n    \"‡§¨‡§π‡•Å‡§§ ‡§¨‡•á‡§ï‡§æ‡§∞! üò§üí¢\",\n    \"Á¥†Êô¥„Çâ„Åó„ÅÑ„Å≠ üòäüëç\"\n]\n\nprint(f\"Testing {len(youtube_comments)} YouTube comments with XLM-RoBERTa\")\n\n# Test XLM-RoBERTa and store results\nresults = []\nfor comment in youtube_comments:\n    true_label = -1  # Unknown, unless you annotate manually\n    xlm_pred, xlm_prob = predict_toxicity(xlm_model, xlm_tokenizer, comment, device)\n    \n    results.append({\n        \"Text\": comment,\n        \"True_Label\": true_label,\n        \"XLM_RoBERTa_Pred\": xlm_pred,\n        \"XLM_RoBERTa_Toxic_Prob\": xlm_prob\n    })\n\n# Convert to DataFrame and save to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/kaggle/working/youtube_comment_test_results_xlm_only.csv\", index=False)\nprint(\"Results saved to /kaggle/working/youtube_comment_test_results_xlm_only.csv\")\nprint(results_df)  # Display full results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T05:16:25.303461Z","iopub.execute_input":"2025-03-17T05:16:25.303811Z","iopub.status.idle":"2025-03-17T05:16:27.705550Z","shell.execute_reply.started":"2025-03-17T05:16:25.303780Z","shell.execute_reply":"2025-03-17T05:16:27.704657Z"}},"outputs":[{"name":"stdout","text":"Testing 16 YouTube comments with XLM-RoBERTa\nResults saved to /kaggle/working/youtube_comment_test_results_xlm_only.csv\n                             Text  True_Label  XLM_RoBERTa_Pred  \\\n0          This video is terrible          -1                 0   \n1        Really well made content          -1                 0   \n2   You should quit making videos          -1                 0   \n3     Thanks for the great upload          -1                 0   \n4             Worst thing ever üòÇüëé          -1                 0   \n5           Love this so much! üòç‚ú®          -1                 0   \n6          Go jump off a cliff ‚ò†Ô∏è          -1                 1   \n7                Awesome work! üòäüëç          -1                 0   \n8            ¬°Qu√© video tan malo!          -1                 0   \n9             Bon contenu, merci!          -1                 0   \n10          ‡§Ü‡§™‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§π‡•à          -1                 0   \n11                      „Åì„ÅÆ„Éì„Éá„Ç™„ÅØ„Å≤„Å©„ÅÑ          -1                 0   \n12              ¬°Eres un genio! ‚≠ê          -1                 0   \n13                   Tu es nul üò°üëä          -1                 1   \n14                 ‡§¨‡§π‡•Å‡§§ ‡§¨‡•á‡§ï‡§æ‡§∞! üò§üí¢          -1                 0   \n15                      Á¥†Êô¥„Çâ„Åó„ÅÑ„Å≠ üòäüëç          -1                 0   \n\n    XLM_RoBERTa_Toxic_Prob  \n0                 0.011366  \n1                 0.011096  \n2                 0.454345  \n3                 0.007368  \n4                 0.383334  \n5                 0.010621  \n6                 0.910009  \n7                 0.009125  \n8                 0.019045  \n9                 0.008321  \n10                0.012220  \n11                0.014252  \n12                0.381864  \n13                0.979940  \n14                0.084865  \n15                0.008955  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T05:32:18.367985Z","iopub.execute_input":"2025-03-17T05:32:18.368333Z","iopub.status.idle":"2025-03-17T05:32:18.393299Z","shell.execute_reply.started":"2025-03-17T05:32:18.368288Z","shell.execute_reply":"2025-03-17T05:32:18.392251Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b8752c95a798>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mount Google Drive (works in Kaggle with authentication)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Follow prompts to authenticate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."],"ename":"NotImplementedError","evalue":"Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.","output_type":"error"}],"execution_count":16}]}